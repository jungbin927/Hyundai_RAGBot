# -*- coding: utf-8 -*-
"""base2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eCE21f--9nE2P8jVHD_dEl72kGZ8sqaz

1. 라이브러리 불러오기
"""

import pandas as pd
import numpy as np
# PDF 처리
import pymupdf # PDF에서 텍스트, 이미지, 테이블을 추출하는 라이브러리
import pdfplumber # PDF에서 표추출을 정밀하게 하는 라이브러리
# 이미지 처리
from PIL import Image # 이미지 처리
import io
import base64

"""2. PDF text 처리"""

pdf_path = "Car_Manual/casper_Manual.pdf"
doc = pymupdf.open(pdf_path)

# 페이지별로 텍스트 추출
page_texts = [] # 텍스트 저장 리스트
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    text = page.get_text("text") # text를 문자열 형식으로 추출
    page_texts.append(text)

# 각 페이지별 텍스트 출력
for i, text in enumerate(page_texts):
    print(f"--- Page {i+1} ---")
    print(text)

print(page_texts[24]) # 25 page 출력

import re

def clean_text(text):
    # 1️⃣ 점(dot)과 구분선 제거
    cleaned_text = re.sub(r'[-=._]{5,}', '', text)

    # 2️⃣ 페이지 번호 형식 제거 (예: "2-10", "3:15", "5-11")
    cleaned_text = re.sub(r'\d{1,3}[-:]\d{1,3}', '', cleaned_text)

    # 3️⃣ 목차 스타일 제거 (예: "2 기타 주의 사항..........", "1-5")
    cleaned_text = re.sub(r'\d+\s*[가-힣\s]+[.\-_=]+', '', cleaned_text)

    # 4️⃣ 불필요한 숫자 제거 (단독 숫자 라인)
    cleaned_text = re.sub(r'^\d+$', '', cleaned_text, flags=re.MULTILINE)

    # 5️⃣ 여러 개의 공백과 줄바꿈을 하나로 통합
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

     # 1️⃣ 특수 문자 및 유니코드 문자가 포함된 부분을 제거
    cleaned_text = re.sub(r'[^\w\sㄱ-ㅎ가-힣]', '', text)  # 한글, 영어, 숫자, 공백 제외한 문자 제거

    return cleaned_text

# ✅ 모든 페이지 텍스트 전처리
preprocessed_texts = [clean_text(page) for page in page_texts]

"""2-2. 청크 처리"""

# 청크 처리
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize # 문장 단위로 텍스트를 나누는 함수

def chunk_by_paragraph(preprocessed_texts, min_length=300):
    """
    문서를 문단 단위로 나누되, 너무 짧은 문단은 다음 문단과 합치는 함수
    """
    paragraphs = "\n\n".join(preprocessed_texts).split("\n\n")
    # 문단 기준으로 나누기
    # 텍스트에서 빈 줄이 두번 이상 나오면 문단 구분
    chunks = [] # 청크 저장 리스트
    current_chunk = ""

    for para in paragraphs:
        if len(current_chunk) + len(para) < min_length:
            current_chunk += " " + para  # 너무 짧으면 이전 문단과 합침
        else:
            chunks.append(current_chunk.strip())  # 적당한 길이면 저장
            current_chunk = para

    if current_chunk:
        chunks.append(current_chunk.strip())  # 마지막 문단 추가

    return chunks

# 적용하기
chunked_texts = []
for page_text in page_texts:  # 페이지별로 분할
    chunked_texts.extend(chunk_by_paragraph(page_text))

"""2-3. 청크 임베딩"""

# SentenceTransformer는 BERT 기반 모델 사용
from sentence_transformers import SentenceTransformer

# 모델 로드 (예: "all-MiniLM-L6-v2" 모델 사용)
model = SentenceTransformer('all-MiniLM-L6-v2')
# 모든 종류의 텍스트에 대해 비교적 고르게 잘 작동하며 속도와 성능의 균형이 잘 맞음

# 문단 단위로 임베딩 생성
embeddings = []
for chunk in chunked_texts:
    embedding = model.encode(chunk)  # 각 문단을 벡터로 변환
    embeddings.append(embedding)

"""2-4. 벡터 DB 생성 (faiss)"""

import faiss
import numpy as np

# 임베딩 벡터를 NumPy 배열로 변환
embeddings_array = np.array(embeddings)

# FAISS 인덱스를 생성
dimension = embeddings_array.shape[1]  # 벡터 차원 (모델에 따라 달라짐)
index = faiss.IndexFlatL2(dimension)  # L2 거리 기반 인덱스 생성

# 벡터 추가 (FAISS 인덱스에 임베딩 벡터 저장)
index.add(embeddings_array)

# 벡터 DB 저장
faiss.write_index(index, 'vector_db2.index')

"""벡터 db 초기화 후 다시 실행"""

import faiss
import numpy as np

# 1️⃣ 기존 벡터 DB 초기화 (이전 데이터 제거)
index.reset()  # FAISS의 경우

# 2️⃣ 새로운 청크 적용
chunked_texts = chunk_by_paragraph(preprocessed_texts)  # 정제된 데이터로 청크 생성

# 3️⃣ 새로운 임베딩 계산
new_embeddings = [model.encode(chunk) for chunk in chunked_texts if len(chunk) > 5]

# 4️⃣ 벡터를 정규화하여 코사인 유사도를 사용하도록 준비
# 정규화 (벡터의 크기를 1로 만듦, 코사인 유사도 계산을 위해)
normalized_embeddings = np.array(new_embeddings).astype('float32')
faiss.normalize_L2(normalized_embeddings)  # L2 정규화 적용

# 5️⃣ 벡터 DB에 새롭게 추가
dimension = normalized_embeddings.shape[1]  # 벡터 차원
index = faiss.IndexFlatIP(dimension)  # 코사인 유사도 = 내적 (IP)

# 벡터 추가
index.add(normalized_embeddings)

# 6️⃣ 벡터 DB 저장
faiss.write_index(index, 'vector_db2.index')

# 7️⃣ 검색 테스트
query = "타이어 펑크났어. 어떻게 해야해?"
query_embedding = model.encode(query).reshape(1, -1)

# 쿼리 벡터 정규화
faiss.normalize_L2(query_embedding)

# 검색 실행 (상위 3개 검색)
distances, indices = index.search(query_embedding, k=3)

# 결과 출력
for i, idx in enumerate(indices[0]):
    print(f"유사도: {distances[0][i]:.4f}")
    print(chunked_texts[idx])

"""3. Streamlit"""

